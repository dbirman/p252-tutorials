---
title: "Introduction to Statistical Computing with R: Module 2"
output:
  html_document:
    highlight: pygments
    theme: flatly
    toc: yes
---

Based on notes by Paul Thibodeau (2009) and revisions by the Psych 252 instructors in 2010, 2011, and 2015

Expanded in 2012 by Mike Frank, Benoit Monin and Ewart Thomas

Converted to [R Markdown](http://www.rstudio.com/ide/docs/r_markdown) format and further expanded in 2013 by Michael Waskom.

```{r}
library(ggplot2)
```


### Welcome!

Welcome to Module 2! If you are in class 1 the second module is designed to get you up to speed on some statistical concepts, and then to let you free on some datasets. If you are in class 2 this first module is a crash course in R!

### A Reminder about DataFrames

In R we almost exclusively use "DataFrames" to organize our data. Here's an example dataframe and some useful functions for investigating what's on the inside.

```{r}
mydata = data.frame(x=c(1,2,3,4,5),y=c(25,94,32,45,88))
mydata
str(mydata)
head(mydata)
summary(mydata)
```

Let's jump right in!

### An interlude on R data

Although it's possible to create a dataframe from scratch (as demonstrated above) in most cases you'll be reading data into R that was created elsewhere. It's useful at this point to introduce two concepts that govern how R thinks about accessing data. When dealing with data that are saved in a file somewhere on your computer, R has the concept of the *working directory*. Any functions that read or write files to or from the disk will take as an argument a filename, and the filename you give should be a path relative to your working directory. You can change the working directory either by calling the `setwd()` function or by using the GUI tools in the R or RStudio apps.

Assigning some value to a variable creates a new object in the *workspace*, which you can think of as R's "working memory." Any object in the workspace can be immediately referenced in a line of code. You can open a pane in RStudio that will show you the name of every object in your workspace along with some information about those objects, and you can also get a vector of these names with the `ls()` function. To remove an object from your workspace, use the `rm()` function.

A sidenote on data storage: Most of the data we'll be using is in *csv* format, which stands for "comma separated values." This is a plain-text format where commas divide columns and rows are placed on new lines. Because the data are stored as plain text, you can view (and edit) them in a basic text editor. The csv format is also advantageous relative to proprietary binary formats (like `.xlsx` or `.mat`) because pretty much any statistical application will contain routines to read and write these files.


Manipulating Dataframes
-----------------------

To begin, we'll load in a simple data file that we'll use in 252, and store it as a [dataframe](http://www.r-tutor.com/r-introduction/data-frame) named "df_survey".

This data file is called `fieldsimul1.csv`. Here, likely voters (*n* = 240) were surveyed, and report their age (`age`), and level of optimism (`optmism`), as well as other variables. **Is age related to optimism?**

```{r load df_survey}
df_survey = read.csv('http://stanford.edu/class/psych252/data/fieldsimul1.csv')
```

Let's explore this data a bit to see what we'll be working with:

```{r explore_df_survey}
str(df_survey)
head(df_survey)
summary(df_survey)
```

Here we can see that this **dataframe** is composed of 7 **variables** that each has 200 observations. In this case, each observation represents a different participant, and the 7 variables tell us information about that participant. As an example, let's look at all the information for participant 5 by looking at the 5th row of df_survey:

```{r observeRow_df_survey}
participant_num = 5
df_survey[participant_num,]
```

This shows us that participant 5 is 25 years old, and has an optimism score of 6.


Integer vs. Factor
------------------

Now, we might want to treat some variables as qualitative, nominal **factors** rather than continuous, numeric **integers**. In R, we must specify which variables to treat as factors if the **levels** (i.e., unique values) of the variable are composed of numbers instead of strings. Note that if the variable (e.g., "Subid") *levels* start with a letter (e.g., "subject1", "subject2") R will automatically interpret the variable as a *factor*. If the variable levels start with a number (e.g., "1", "2"), R will automatically interpret the variable as an *integer*. If you want the variable interpreted differently, you have to tell R.

For instance, the variable "age" is continuous, but "agecat" is not. However, since the **levels** of "agecat" are indicated with numbers, we must tell R to treat "agecat" as a factor:

```{r agecat_asfactor_df_survey}
df_survey$agecat = factor(df_survey$agecat)
```

Now we can look at the structure of the df_survey dataframe again, to make sure agecat is now a factor:

```{r agecat_checkfactor_df_survey}
str(df_survey)
```

We can also plot age as a function of age category. Here we'll tell R to make the plot *with* the df_survey dataframe. This saves on typing "df_survey$" in front of each variable name.


Basic Plotting
--------------

If you're starting in Module 2 directly we are using a package called `ggplot()` for plotting, call `install.packages('ggplot2')` to get the package, and use it as shown below. Note the "layered" structure, where we add the boxplot on top of the base object.

Let's take a look at the factor we created, by plotting age category as a function of age.

```{r plot_agecat, fig.width=7, fig.height=6}
ggplot(data=df_survey) +
  geom_boxplot(aes(x=agecat,y=age))
```

Creating factors from continuous variables
------------------------------------------

Suppose the variable "agecat" wasn't given to us, but we still wanted to group participants into categories based on their respective ages. Given the "age" variable, we can create a new categorical variable (i.e., **factor**) by specifying breaks at specific intervals:

```{r agecat0_df_survey}
df_survey$agecat0 = findInterval(df_survey$age, c(29,39,49,65))
df_survey$agecat0 = factor(df_survey$agecat0)
head(df_survey)
```

Specifically, these break points result in 5 age categories, 0:28, 29:38, 39:48, 49:64, and 65 and up. We can also visualize these groups:

```{r plot_agecat0, fig.width=5, fig.height=4}
ggplot(data=df_survey) +
  geom_boxplot(aes(x=agecat0,y=age))
```

Let's also take a look at plotting a histogram of the ages. Here we are also setting the color of the bars as well as the title of the plot. The BW theme removes that ugly grey grid in the background which is the default.

```{r plot_agedist, fig.width=5, fig.height=4}
ggplot(data=df_survey) +
  geom_histogram(aes(x=age),fill='orange',color='black',binwidth=4) + 
  ggtitle('Distribution of Age') +
  theme_bw()
```

Next, let's view a scatter plot of optimism by age. Before when we plotted the data, we were plotting a **factor** variable on the x-axis. As a result, boxplots were the logical way to view the data. However, if the x-axis variable is **continuous** (i.e., numeric), we would prefer a scatterplot.

We'll also add in a line to get a sense of the general trend of the data; here, we'll plot a non-parametric best-fitting curve in red using the ["lowess"](http://rss.acs.unt.edu/Rdoc/library/gplots/html/lowess.html) function.

```{r plot_optmism_on_age, fig.width=5, fig.height=4}
ggplot(data=df_survey,aes(x=age,y=optmism)) +
  ggtitle('Optimism vs. Age') +
  geom_point() +
  geom_smooth(method="lm",color="red")
```

Linear Regression
------------------

To examine the best-fitting linear relationship between quantitative variables (e.g., X and Y), we can use the function [`lm()`](http://www.statmethods.net/stats/regression.html). We'll be using this function very often throughout the class to fit linear models (which as we will see is a very broad category of statistical analysis). To use `lm()`, you must provide a formula, like `Y ~ X`. In this case, X is your *independent variable* (IV), and Y is your *dependent variable* (DV). That is, this formula asked "as X changes, what happens to Y? How much of the variance in Y is explained by variance in X?"

For instance, we might want to investigate the relationship between age (X) and optimism (Y), depicted in the plot above. As age increases, how does optimism change?

```{r lm_ageVSoptmism_df_survey}
rs2 = lm(optmism ~ age, data = df_survey) # this is the same as lm(df_survey$optmism ~ df_survey$age)
summary(rs2) # given the model we just fit with lm(), give us the summary
```

Based off of this output, we can see that age is a significant predictor of optimism; the variance in age explains approximately 10% of the variance in optimism.

We can also plot the regression line, fit by least squares regression, on top of the scatterplot. Here, we use the function **abline()**, and give it the model we just fit above with lm(). While we're doing this, let's also change the names of the x- and y-axis labels:

```{r plot_optmism_on_age_reg, fig.width=5, fig.height=4}
plot(df_survey$optmism ~ df_survey$age, main = 'Optimism vs Age', xlab='Age', ylab='Optimism')
abline(rs2, col='red')
```

We can also visualize diagnostic plots of the residuals from the lm() output, which  can be helpful in showing if assumptions of normality, etc, are met, or to identify outliers:

```{r plot_optmism_on_age_lm, fig.width=5, fig.height=4}
plot(rs2)
```

Reading data from online repositories
--------------------------------------

R allows us to read data that's stored in a file on your computer or from a URL online. Here, we're going to look at data where each participant (*n* = 50) reports whether they took a flu shot (`Shot` = 1) or not (`Shot` = 0), their age (`Age`), and their health awareness level (`Health.Aware`).  Is Prob(`Shot` = 1) influenced by `Age`?  By `Health.Aware`?

We can provide the website URL, and R will read in the table:

```{r flu_load}
flu.table <- read.table('http://stanford.edu/class/psych252/data/flu.table', header=TRUE)

str(flu.table)    # description of dataframe
```

Note that when you're reading in a **table**, you have to tell R that you want the first row in the file to be the header (i.e., the names of the variables) with the option "header=TRUE". If you have a **csv** R will treat the first row as a header by default.

Here, we have a numeric matrix of data, with 50 observations for 3 variables. To get an idea of the pairwise relationships between all these variables, we can call `plot()` on the DataFrame. This is another example of how `plot()` is a powerful generic function:

```{r plot_flu_df, fig.width=5, fig.height=4}
plot(flu.table,cex.labels=2, pch=23, bg='orange', cex=2)
```

It looks like the probability of receiving a shot might be related to the amount of health awareness. Let's view a scatterplot of just these two variables, and overlay a red non-parametric best-fitting curve showing how the Probability(Shot = 1) varies with health awareness:

```{r plot_shot_on_aware, fig.width=5, fig.height=4}
plot(flu.table$Health.Aware, flu.table$Shot, main = 'Prob(Shot) vs Awareness',
     xlab = 'Health Awareness', ylab = 'Shot Probability')
lines(lowess(flu.table$Health.Aware, flu.table$Shot), lty = 2, col='red')  # 
```

More example data
-----------------

Now, we'll load in some data on several cities (*n* = 38), including whether each city has a right-to-work law (`RTWL` = 1) or not (`RTWL` = 0), its cost-of-living (`COL`), population (`Pop`), population density (`PD)`, unemployment rate (`URate`), taxes (`Taxes`), and income (`Income`). **Is COL influenced by RTWL?**  [RTWL's, e.g., Taft-Hartley Amendments (1947), tended to reduce the power of unions over workers; did they make life easier or harder for workers?]  **Does RTWL affect COL, after controlling for other possibly relevant variables?**

These data are stored in a table where columns are separated by a tab (which is represented by the `\t` code. We'll tell `read.csv()` to be expecting tabs where it by default expects commas: 

```{r rtw_load}
rtw.table <- read.csv("http://stanford.edu/class/psych252/data/P005.txt", sep='\t')

str(rtw.table)
```

Let's plot a boxplot of the cost of living (COL) by whether or not there is a right-to-work law in place. Whether or not a city has RTWLs is coded in these data with '0' and '1'. That's fine for a lot of uses, but we're going to convert those values to *logical* (aka *boolean*) values, `FALSE` and `TRUE`, respectively, so the x-axis labels are more intuitive.

```{r plot_col_on_rtwl, fig.width=5, fig.height=4}
rtw.table$RTWL = as.logical(rtw.table$RTWL)
boxplot(rtw.table$COL ~ rtw.table$RTWL, col='orange', pch=23, bg='red',main='Cost of Living as a Function of Right to Work Laws')
```


Performing t-tests in R
-----------------------

The *t-test* allows us to test a **null hypothesis ($H_0$)** about population means. For instance, if we are interested in whether two populations are different, we can test the $H_0$ that the mean of population 1 is equal to the mean of population 2.

As an example, we'll return to some data from Tutorial 2. This data has information about whether or not cities have right to work laws (RTWL), and their cost of living (COL). Here, we might be interested in whether the right to work law has an influence on cost of living. In this case, our null hypothesis ($H_0$) is that the mean COL is the same in cities with RTWL = 0 as in those with RTWL = 1.

In this case, it is important to note that the 2 populations of cities are *independent* of each other. That is, the cities with no RTWL are different from the cities that have a RTWL.

To test our $H_0$, we'll compute a "standardized difference", or **t statistic**, between the *sample mean* COL of the cities with RTWL=0 and the *sample mean* COL of the cities with RTWL=1. Once we compute this t, we can decide if it's "large" (e.g., t < -2 or t > 2) to determine if we can reject our $H_0$. For now, you don't have to worry much about these details; we'll cover them in the first few classes!

### Exploring the data

Let's start by reading in our data:

```{r load_df_RTWL}
df_RTWL = read.csv("http://stanford.edu/class/psych252/data/P005.txt", sep='\t')
str(df_RTWL)
head(df_RTWL)
```

Now we'll plot the data to see the relationship between RTWL and COL:

```{r plot_col_v_rtwl, fig.width=4, fig.height=3}
boxplot(COL ~ RTWL, col='yellow', 
        pch=23, 
        xlab = "RTWL", ylab = "COL",
        main='COL vs RTWL',
        data=df_RTWL)
```

### Short-form data for t.test()

Before we can run a t-test, we have to make some changes to the data. From looking at the data above, we can see that the dataframe df_RTWL is in *long-form*; each variable is in a different column. This format is fine for `boxplot()` and `lm()`, but older functions like `t.test()` need the data in *short-form* with COL for cities with RTWL = 0 in one column, and COL for cities with RTWL = 1 in a 2nd column.

We'll make 2 variables by selecting **subsets** of the COL data, *conditional* on whether or not the cities have a RTWL. In R, the *brackets* (`[ ]`) let us select rows of a variable conditional on another variable. Here, we'll select the rows of COL where RTWL is equal to 0, or 1:

```{r subsetCOL_df_RTWL}
COL0 = df_RTWL$COL[df_RTWL$RTWL == 0]
COL1 = df_RTWL$COL[df_RTWL$RTWL == 1]
```

Now that the COL data is grouped into variables based on RTWL status, we can explore these variables quickly:

```{r explore_subsetCOL_df_RTWL}
str(COL0)
str(COL1)
```

Here, we see that there are 28 cities with RTWL=0, and 10 cities with RTWL=1.

### Running an Independent Samples t-test

Now that we have our data in the correct format, we can run a t-test testing the H0 that mean(COL0) = mean(COL1). Right now, we'll just assume that the two variances are equal.

```{r ttest_subsetCOL_df_RTWL}
res1 = t.test(COL0, COL1, var.equal = TRUE)
res1
```

### Testing for equal variance

Great! However, we automatically assumed that the variances were equal between the 2 groups of cities. We can use the **standard deviation** function `sd()` to estimate the standard deviation for these 2 groups:

```{r sd_subsetCOL_df_RTWL}
c(sdf_RTWL = sd(COL0), sd1 = sd(COL1))
```

These numbers are pretty different; however, the two groups had different numbers of cities. We can use the `var.test()` function to directly test the H0 that the variance between two samples is equal.

```{r vartest_subsetCOL_df_RTWL}
var.test(COL0, COL1)
```

Here, we cannot reject the null because the two variances are not significantly different from each other (*p* = 0.5063).

### Running a Paired t-test

Suppose the COL measurements for RTWL=0 and RTWL=0 had been taken from the *same* cities. That is, the COL when RTWL=0 is from *before* the law was passed; the COL when RTWL=1 is from *after* the law was passed. Now, the samples would NOT be independent. Instead, the COL when RTWL=1 in city A is *dependent* on COL when RTWL=0. In this case, the two samples are *paired*. **Is COL influenced by RTWL?**

As an example, we have a study in which, for each city, COL is measured BEFORE the RTWL is passed, and 2 years AFTER the RTWL is passed:

```{r load_df_RTWLs}
df_RTWLs = read.csv('http://stanford.edu/class/psych252/data/rtwl.paired.csv')
str(df_RTWLs)
```

Since the two groups are **paired** (i.e., NOT independent), we must specify that in the `t.test()` by saying "`paired=TRUE`". Otherwise, our results will be incorrect.

```{r ttest_df_RTWLs}
res2a = t.test(df_RTWLs$col0, df_RTWLs$col1, 
               var.equal = TRUE, 
               paired = TRUE)
res2a
```

If we made a mistake and forgot to specify "`paired=TRUE`", R would default to `paired=FALSE`, and our output would be incorrect:

```{r ttest_incorr_df_RTWLs}
res2b = t.test(df_RTWLs$col0, df_RTWLs$col1, 
               var.equal = TRUE)
res2b
```

Here, the difference is not quite significant! So it's important you tell R that the groups are paired when the two samples are dependent on one another!